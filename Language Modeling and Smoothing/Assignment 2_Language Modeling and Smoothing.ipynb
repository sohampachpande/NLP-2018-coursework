{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.util import ngrams\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assistance functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to find n Choose r\n",
    "def nCr(n,r):\n",
    "    r2 = max(r,n-r)\n",
    "    ncr = 1\n",
    "    for m in range(n,r2,-1):\n",
    "        ncr = ncr*m\n",
    "    for m in range(1,n-r2+1):\n",
    "        ncr = ncr//m\n",
    "    return ncr\n",
    "\n",
    "\n",
    "# split sentence\n",
    "def sent_splitter(sent, add_tag=1):\n",
    "    if type(sent)==list or type(sent)==tuple:\n",
    "        return sent\n",
    "    elif type(sent)==str:\n",
    "        punctuation_char = string.punctuation + '“”'\n",
    "        sent = sent.translate(str.maketrans('','',punctuation_char))\n",
    "        if add_tag:\n",
    "            sent = '<s> '+ sent.lower()+ ' </s>'\n",
    "        sent = sent.split(sep=' ')\n",
    "        return sent\n",
    "    else:\n",
    "        print(sent)\n",
    "        print(\"Invalid data passed to sentence splitter\")\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing of Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('theAdventuresofSherlockHolmes.txt',encoding=\"utf8\")\n",
    "# f = open('AliceAdventuresinWonderland.txt',encoding=\"utf8\")\n",
    "\n",
    "corpus = f.read()\n",
    "corpus = corpus.replace(\"'\", ' ')\n",
    "corpus = corpus.replace(\"-\", ' ')\n",
    "# Removing unneccessay punctuation characters except .,?,!\n",
    "punctuation_char = string.punctuation + '“”'\n",
    "punctuation_char = punctuation_char.replace('!','')\n",
    "punctuation_char = punctuation_char.replace('?','')\n",
    "punctuation_char = punctuation_char.replace('.','')\n",
    "corpus = corpus.translate(str.maketrans('','',punctuation_char))\n",
    "corpus = corpus.replace('--', '')\n",
    "# Some unneccessary Roman Numericals\n",
    "corpus = corpus.replace('\\nI.', '')\n",
    "corpus = corpus.replace('\\nII.', '')\n",
    "corpus = corpus.replace('\\nIII.', '')\n",
    "# Removing new line characters\n",
    "corpus = corpus.replace('\\n', ' ')\n",
    "\n",
    "# Splitting Corpus into Sentences\n",
    "sent_corpus = nltk.tokenize.sent_tokenize(corpus)\n",
    "\n",
    "# Adding start and end of sentence tags\n",
    "for i in range(len(sent_corpus)):\n",
    "    sent_corpus[i] = '<s> '+ sent_corpus[i].replace('.','').lower()+ ' </s>'     \n",
    "\n",
    "# Splitting Sentences into words\n",
    "words_sent_corpus = []\n",
    "# count_words = 0\n",
    "for i in range(len(sent_corpus)):\n",
    "    words_sent_corpus.append(sent_corpus[i].split(sep=' '))\n",
    "#     count_words += len(sent_corpus[i].split(sep=' '))-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spliting Corpus 80:20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = len(words_sent_corpus)\n",
    "words_sent_corpus_testing = words_sent_corpus[math.ceil(0.8*l):]\n",
    "words_sent_corpus = words_sent_corpus[:math.ceil(0.8*l)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding n-grams and their counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_ngram(n):\n",
    "    list_ngram = []\n",
    "    for w in words_sent_corpus:\n",
    "        for w2 in list(ngrams(w,n)):\n",
    "            if n==1:\n",
    "                list_ngram.append((w2))\n",
    "            else:\n",
    "                if w2!=('','','','') and w2!=('','','') and w2!=('','') and w2!=(''):\n",
    "                    list_ngram.append(w2)\n",
    "    return list_ngram\n",
    "\n",
    "dict_ngram={} # key= n, value= ngram\n",
    "for i in range(1,5):\n",
    "    dict_ngram.update({i:find_ngram(i)})\n",
    "\n",
    "# Count of k word sequence's for k = 1,2,3,4 for kth order MLE\n",
    "# 1-unigram,2-bigram,3-trigram,4-quadgram\n",
    "dict_wordseq_count = {1:{},2:{},3:{},4:{}}  \n",
    "def form_count_for_MLE(n):\n",
    "    global dict_wordseq_count\n",
    "    if len(dict_wordseq_count[n])!=0: # to ensure this function isn't executed twice in the notebook\n",
    "        pass\n",
    "    else:\n",
    "        for i in dict_ngram[n]:\n",
    "            try:\n",
    "                dict_wordseq_count[n][i]+=1\n",
    "            except KeyError:\n",
    "                dict_wordseq_count[n].update({i:1})\n",
    "for i in range(1,5):\n",
    "    form_count_for_MLE(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of ngrams present and number of ngrams possible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of 1-gram present (counting repetitions): 94997\n",
      "Number of Unique 1-gram: 7598\n",
      "Number of 1-gram possible: 7598\n",
      "\n",
      "Number of 2-gram present (counting repetitions): 89368\n",
      "Number of Unique 2-gram: 42124\n",
      "Number of 2-gram possible: 887194626\n",
      "\n",
      "Number of 3-gram present (counting repetitions): 83961\n",
      "Number of Unique 3-gram: 69349\n",
      "Number of 3-gram possible: 55584099100474\n",
      "\n",
      "Number of 4-gram present (counting repetitions): 78540\n",
      "Number of Unique 4-gram: 75112\n",
      "Number of 4-gram possible: 1326146093198017070\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,5):\n",
    "    print(\"Number of {}-gram present (counting repetitions): {}\".format(str(i),len(dict_ngram[i])))\n",
    "    print(\"Number of Unique {}-gram: {}\".format(str(i),len(dict_wordseq_count[i])))\n",
    "    print(\"Number of {}-gram possible: {}\\n\".format(str(i),nCr(len(dict_wordseq_count[i]),i)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum likelihood Estimate for ngram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Function to find probabity of given word sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Sentences with Probability:\n",
      "\n",
      "Sentence: visit\n",
      "\n",
      "1-Gram Model, Probability:9.473983388949125e-05\n",
      "\n",
      "Sentence: i sat\n",
      "\n",
      "2-Gram Model, Probability:0.0022271714922048997\n",
      "\n",
      "Sentence: do you mean\n",
      "\n",
      "3-Gram Model, Probability:0.045454545454545456\n",
      "\n",
      "Sentence: i had got when\n",
      "\n",
      "4-Gram Model, Probability:1.0\n"
     ]
    }
   ],
   "source": [
    "def mle_p_ngram(n,wlist):\n",
    "#     if type(wlist)==str:\n",
    "    wlist = sent_splitter(wlist,add_tag=0)\n",
    "        \n",
    "    if len(wlist)!=n:\n",
    "        print(\"Invalid model, number of words is not equal to {}\".format(n))\n",
    "        return -1\n",
    "    try:  \n",
    "        if n==1:\n",
    "            return float(dict_wordseq_count[n][(wlist[0],)])/(len(dict_ngram[n]))\n",
    "        elif n==2:\n",
    "            return float(dict_wordseq_count[n][tuple(wlist)])/dict_wordseq_count[n-1][(wlist[0],)]\n",
    "        elif n==3:\n",
    "            return float(dict_wordseq_count[n][tuple(wlist)])/dict_wordseq_count[n-1][wlist[0],wlist[1]]\n",
    "        elif n==4:\n",
    "            return float(dict_wordseq_count[n][tuple(wlist)])/dict_wordseq_count[n-1][wlist[0],wlist[1],wlist[2]]\n",
    "    except KeyError: #Word not found\n",
    "        return 0\n",
    "\n",
    "n=2\n",
    "w = 'with an'\n",
    "# mle_p_ngram(n,w)\n",
    "\n",
    "print(\"Example Sentences with Probability:\")\n",
    "sl = ['visit','i sat','do you mean','i had got when']\n",
    "for i in range(4):\n",
    "    print('\\nSentence: {}\\n'.format(sl[i]))\n",
    "    try:\n",
    "        print(\"{}-Gram Model, Probability:{}\".format(i+1,mle_p_ngram(i+1,sl[i])))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find probability for all ngrams present in corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mle_word_dict = {1:{},2:{},3:{},4:{}}\n",
    "for n in range(1,5):\n",
    "    for w in dict_wordseq_count[n]:\n",
    "            try:\n",
    "                mle_word_dict[n][w] += mle_p_ngram(n,w)\n",
    "            except KeyError:\n",
    "                mle_word_dict[n].update({w:mle_p_ngram(n,w)})\n",
    "                    \n",
    "mle_wordseq_prob_list = {1:[],2:[],3:[],4:[]}\n",
    "mle_wordseq_list = {1:[],2:[],3:[],4:[]}\n",
    "\n",
    "for n in range(1,5):\n",
    "    for w in mle_word_dict[n]:\n",
    "        mle_wordseq_prob_list[n].append(mle_word_dict[n][w])\n",
    "        mle_wordseq_list[n].append(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to find probability of sentence for given ngram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Sentences with Probability:\n",
      "\n",
      "\n",
      "Sentence: i sat down\n",
      "\n",
      "1-Gram Model, Probability:6.501378758699384e-11\n",
      "2-Gram Model, Probability:2.9039371565473137e-06\n",
      "3-Gram Model, Probability:7.450176941702365e-05\n",
      "\n",
      "Sentence: do you mean that\n",
      "\n",
      "1-Gram Model, Probability:1.4668322521718687e-13\n",
      "2-Gram Model, Probability:9.5934619118052e-09\n",
      "3-Gram Model, Probability:0.0\n",
      "\n",
      "Sentence: the lamps had been lit but the blinds had not been drawn\n",
      "\n",
      "1-Gram Model, Probability:2.524209073501141e-36\n",
      "2-Gram Model, Probability:0.0\n",
      "3-Gram Model, Probability:2.0156754402622197e-10\n",
      "\n",
      "Sentence: so far i had got when we went to visit the scene of action\n",
      "\n",
      "1-Gram Model, Probability:4.4761176669234027e-41\n",
      "2-Gram Model, Probability:1.0186416757951622e-25\n",
      "3-Gram Model, Probability:4.16666666666667e-08\n"
     ]
    }
   ],
   "source": [
    "def probability_sentence(sent,model_no):\n",
    "    sent=sent_splitter(sent,add_tag=1)\n",
    "    prob = 0\n",
    "    if model_no==1:\n",
    "        l = -len(sent)\n",
    "    else:\n",
    "        l = model_no\n",
    "    for w in sent[:-l+1]:\n",
    "        index = sent.index(w)\n",
    "        if index+model_no<=len(sent):\n",
    "            temp = mle_p_ngram(model_no,sent[index:index+model_no])\n",
    "#             print(temp)\n",
    "            if temp==0: # How to take log\n",
    "                probt = float(\"-inf\")\n",
    "            else:\n",
    "                probt = math.log(temp)\n",
    "            prob += probt\n",
    "#             print (prob)\n",
    "    return math.exp(prob)\n",
    "\n",
    "\n",
    "print(\"Example Sentences with Probability:\\n\")\n",
    "sl = ['i sat down','do you mean that','the lamps had been lit but the blinds had not been drawn','so far i had got when we went to visit the scene of action']\n",
    "for s in sl:\n",
    "    print('\\nSentence: {}\\n'.format(s))\n",
    "    for i in range(1,4):\n",
    "        print(\"{}-Gram Model, Probability:{}\".format(i,probability_sentence(s,i)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Sentences generated using NGram Model:\n",
      "\n",
      "\n",
      "1-Gram Model\n",
      "\n",
      "<s> sholto clock? faded bachelor exceedingly 2000 income turn cobwebby commit track creatures 85 avail epistle troubled discriminate four heads nod don aspired wrist pshaw! grey hum!\n",
      "<s> mahogany completely emotions brain commenting overhauled jove! curb like varieties commissions expectancies fat reared intrusted glances eligible hide groomed crisp hawk hercules enigmatical nut requested discover\n",
      "<s> dried wouldn madam? grinned k forgive lure antecedents boasting georgia throat scream fangs wore repeatedly cal trouble! scuffle longed encompass ashen sleeper concisely merit fills dig\n",
      "\n",
      "2-Gram Model\n",
      "\n",
      "<s> fritz! </s>\n",
      "<s> american had come upon those lines at exactly my limits in monosyllables and unclaspings of proof with eager nature rather suddenly and gave a boarding school what mr lestrade rose from off however there may take in bewilderment at anything to affect the continent </s>\n",
      "<s> three pound a whip across is itself implies as akin to illustrate </s>\n",
      "\n",
      "3-Gram Model\n",
      "\n",
      "<s> grit in a few paces of the buckles </s>\n",
      "<s> chubb lock to the death of dr grimesby roylott of stoke moran </s>\n",
      "<s> which dealer s? </s>\n",
      "\n",
      "4-Gram Model\n",
      "\n",
      "<s> won t it ring? </s>\n",
      "<s> without a word he grasped my arm and hurried me into a carriage the night before waiting for me? </s>\n",
      "<s> colonel! </s>\n"
     ]
    }
   ],
   "source": [
    "def prob_ngrams_starting_with_w(w_seq, model_no):\n",
    "    prob_list = []\n",
    "    word_list = []\n",
    "    indexing_list = []\n",
    "    if type(w_seq)==str:\n",
    "        w_seq= sent_splitter(w_seq, add_tag=0)\n",
    "#     print('prob_ngrams_w',w_seq)\n",
    "    for i in range(len(mle_wordseq_list[model_no])):\n",
    "        if model_no in [2,3,4]:\n",
    "            if mle_wordseq_list[model_no][i][:model_no-1] == tuple(w_seq):\n",
    "                indexing_list.append(len(word_list))\n",
    "                word_list.append(mle_wordseq_list[model_no][i])\n",
    "                prob_list.append(mle_wordseq_prob_list[model_no][i])\n",
    "        elif model_no==1:\n",
    "            indexing_list.append(len(word_list))\n",
    "            word_list.append(mle_wordseq_list[model_no][i])\n",
    "            prob_list.append(mle_wordseq_prob_list[model_no][i])            \n",
    "    return word_list, prob_list, indexing_list\n",
    "\n",
    "def sent_generate(model_no):\n",
    "    w1 = '<s>'\n",
    "    we = '</s>'\n",
    "    sfinal = '<s>'\n",
    "    if model_no==1:\n",
    "        w_seq = '<s>'\n",
    "        count = 0\n",
    "        while w_seq != '</s>':\n",
    "            word_list,prob_list,indexing_list = prob_ngrams_starting_with_w(w_seq,model_no)\n",
    "            next_w_index = np.random.choice(indexing_list,1,prob_list)\n",
    "            w_seq = word_list[int(next_w_index)][0]\n",
    "            sfinal += ' '+ w_seq\n",
    "            if count == 25:\n",
    "                break\n",
    "            count +=1\n",
    "        return sfinal\n",
    "    \n",
    "    elif model_no==2:\n",
    "        w_seq = '<s>'\n",
    "        while w_seq != '</s>':    \n",
    "            word_list,prob_list,indexing_list = prob_ngrams_starting_with_w((w_seq,),model_no)\n",
    "            next_w_index = np.random.choice(indexing_list,1,prob_list)\n",
    "            w_seq = word_list[int(next_w_index)][1]\n",
    "            sfinal += ' '+ w_seq\n",
    "        return sfinal\n",
    "    elif model_no==3:\n",
    "        w_seq = ['<s>']\n",
    "        # First get second word from bigram counts. \n",
    "        # This is valid as count('<s>','<s>',w)=count('<s>',w) in our case as we are not taking multiple '<s>'\n",
    "        word_list,prob_list,indexing_list = prob_ngrams_starting_with_w((w_seq[0],),model_no-1)\n",
    "        next_w_index = np.random.choice(indexing_list,1,prob_list)\n",
    "        w_seq.append(word_list[int(next_w_index)][1])\n",
    "#         print(w_seq)\n",
    "        sfinal += ' '+ word_list[int(next_w_index)][-1]\n",
    "        while w_seq[1] != '</s>':\n",
    "            word_list, prob_list, indexing_list = prob_ngrams_starting_with_w(w_seq,model_no)\n",
    "            next_w_index = np.random.choice(indexing_list,1,prob_list)\n",
    "            w_seq = word_list[int(next_w_index)][1:]\n",
    "#             print(w_seq)\n",
    "            sfinal += ' '+ w_seq[-1]\n",
    "        return sfinal\n",
    "    elif model_no==4:\n",
    "        w_seq = ['<s>']\n",
    "        # First get second word from bigram counts. \n",
    "        # This is valid as count('<s>','<s>',w)=count('<s>',w) in our case as we are not taking multiple '<s>'\n",
    "        # Similarly get third word from trigram\n",
    "        word_list,prob_list,indexing_list = prob_ngrams_starting_with_w((w_seq[0],),model_no-2)\n",
    "        next_w_index = np.random.choice(indexing_list,1,prob_list)\n",
    "        w_seq.append(word_list[int(next_w_index)][1])\n",
    "#         print(w_seq)\n",
    "        sfinal += ' '+ word_list[int(next_w_index)][-1]\n",
    "        \n",
    "        word_list, prob_list, indexing_list = prob_ngrams_starting_with_w(w_seq,model_no-1)\n",
    "        next_w_index = np.random.choice(indexing_list,1,prob_list)\n",
    "        w_seq = word_list[int(next_w_index)][:]\n",
    "#         print(w_seq)\n",
    "        sfinal += ' '+ word_list[int(next_w_index)][-1]\n",
    "        \n",
    "        while w_seq[2] != '</s>':\n",
    "            word_list, prob_list, indexing_list = prob_ngrams_starting_with_w(w_seq,model_no)\n",
    "            next_w_index = np.random.choice(indexing_list,1,prob_list)\n",
    "            w_seq = word_list[int(next_w_index)][1:]\n",
    "#             print(w_seq)\n",
    "            sfinal += ' '+ w_seq[-1]\n",
    "        return sfinal\n",
    "        \n",
    "print(\"Example Sentences generated using NGram Model:\\n\")\n",
    "for i in range(1,5):\n",
    "    print('\\n{}-Gram Model\\n'.format(i))\n",
    "    print(sent_generate(i))\n",
    "    print(sent_generate(i))\n",
    "    print(sent_generate(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add-1 smoothing for bigram + Updated count:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples with Drastic change in Counts:\n",
      "\n",
      "Bigram: ('of', 'the')\n",
      "Count: 581\n",
      "Post Add 1 Smoothing Count: 129.71034059527463\n",
      "Post Add 1 Smoothing Probability: 0.05952746241178276\n",
      "\n",
      "Bigram: ('it', 'is')\n",
      "Count: 269\n",
      "Post Add 1 Smoothing Count: 40.45317220543807\n",
      "Post Add 1 Smoothing Probability: 0.030211480362537766\n",
      "\n",
      "Bigram: ('that', 'i')\n",
      "Count: 185\n",
      "Post Add 1 Smoothing Count: 27.920805369127518\n",
      "Post Add 1 Smoothing Probability: 0.02080536912751678\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def add1_bigram_prob(wlist):\n",
    "#     if type(wlist)==str:\n",
    "    wlist=sent_splitter(wlist,add_tag=0)\n",
    "    if len(wlist)!=2:\n",
    "        print(\"Not a Bigram!\")\n",
    "        return -1\n",
    "    else:\n",
    "        try:\n",
    "            return (float(dict_wordseq_count[2][tuple(wlist)])+1)/(dict_wordseq_count[1][(wlist[0],)]+ len(dict_wordseq_count[1]))\n",
    "        except KeyError:\n",
    "            try:\n",
    "                return (1)/(dict_wordseq_count[1][(wlist[0],)]+ len(dict_wordseq_count[1]))\n",
    "            except KeyError:\n",
    "#               (\"The 1st word of bigram doesnt exist in training corpus\".format(wlist[0]))\n",
    "                return 1/(1+len(dict_wordseq_count[1]))\n",
    "def add1_bigram_count(wlist):\n",
    "    if type(wlist)==str:\n",
    "        wlist=sent_splitter(wlist,add_tag=0)\n",
    "    if len(wlist)!=2:\n",
    "        print(\"Not a Bigram!\")\n",
    "        return -1\n",
    "    else:\n",
    "        return dict_wordseq_count[1][(wlist[0],)]*add1_bigram_prob(wlist)\n",
    "    \n",
    "print(\"Examples with Drastic change in Counts:\\n\")\n",
    "wl = [('of','the'),('it', 'is'),('that', 'i')]\n",
    "for w in wl:\n",
    "    print(\"Bigram: {}\\nCount: {}\\nPost Add 1 Smoothing Count: {}\\nPost Add 1 Smoothing Probability: {}\\n\".format(w,dict_wordseq_count[2][tuple(w)],add1_bigram_count(w),add1_bigram_prob(w)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability of Sentence using Bigram with Add-1 smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Sentences with 2-gram model Probability after Add-1 Smoothing:\n",
      "\n",
      "Sentence: i sat down\n",
      "Probability:5.3189834149362046e-11\n",
      "\n",
      "Sentence: do you mean that\n",
      "Probability:5.9503724659307494e-15\n",
      "\n",
      "Sentence: the lamps had been lit but the blinds had not been drawn\n",
      "Probability:2.261365020126562e-40\n",
      "\n",
      "Sentence: so far i had got when we went to visit the scene of action\n",
      "Probability:1.9439558762275912e-47\n"
     ]
    }
   ],
   "source": [
    "def probability_sentence_add1_bigram(sent):\n",
    "    sent=sent_splitter(sent,add_tag=1)\n",
    "    prob = 0\n",
    "    for w in sent[:-1]:\n",
    "        index = sent.index(w)\n",
    "        if index+2<=len(sent):\n",
    "            try:\n",
    "                temp = add1_bigram_prob(sent[index:index+2])\n",
    "            except KeyError:\n",
    "                temp=0                \n",
    "#             print(temp)\n",
    "            if temp==0: # How to take log----take -inf\n",
    "                probt = float(\"-inf\")\n",
    "            else:\n",
    "                probt = math.log(temp)\n",
    "            prob += probt\n",
    "#             print (prob)\n",
    "    return math.exp(prob)\n",
    "\n",
    "\n",
    "print(\"Example Sentences with 2-gram model Probability after Add-1 Smoothing:\")\n",
    "sl = ['i sat down','do you mean that','the lamps had been lit but the blinds had not been drawn','so far i had got when we went to visit the scene of action']\n",
    "for s in sl:\n",
    "    print('\\nSentence: {}'.format(s))\n",
    "    print(\"Probability:{}\".format(probability_sentence_add1_bigram(s)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Good Turing Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1-Gram Model:\n",
      "\n",
      "\tCount= 1 -> Good Turing Count= 0.6346782988004362\n",
      "\tCount= 2 -> Good Turing Count= 1.6546391752577319\n",
      "\tCount= 3 -> Good Turing Count= 2.2866043613707165\n",
      "\tCount= 4 -> Good Turing Count= 3.7329700272479562\n",
      "\tCount= 5 -> Good Turing Count= 4.532846715328467\n",
      "\tCount= 6 -> Good Turing Count= 4.8019323671497585\n",
      "\tCount= 7 -> Good Turing Count= 7.154929577464789\n",
      "\tCount= 8 -> Good Turing Count= 7.086614173228346\n",
      "\tCount= 9 -> Good Turing Count= 7.5\n",
      "\tCount= 10 -> Good Turing Count= 9.68\n",
      "\n",
      " Discounting value d for 1-gram= 0.5934785304151798\n",
      "\n",
      "\n",
      "2-Gram Model:\n",
      "\n",
      "\tCount= 1 -> Good Turing Count= 0.32000756072204894\n",
      "\tCount= 2 -> Good Turing Count= 1.1139988186650915\n",
      "\tCount= 3 -> Good Turing Count= 1.9533404029692472\n",
      "\tCount= 4 -> Good Turing Count= 2.8718783930510314\n",
      "\tCount= 5 -> Good Turing Count= 4.378071833648393\n",
      "\tCount= 6 -> Good Turing Count= 4.461139896373057\n",
      "\tCount= 7 -> Good Turing Count= 6.861788617886178\n",
      "\tCount= 8 -> Good Turing Count= 6.5260663507109005\n",
      "\tCount= 9 -> Good Turing Count= 8.03921568627451\n",
      "\tCount= 10 -> Good Turing Count= 7.780487804878049\n",
      "\n",
      " Discounting value d for 2-gram= 1.0694004634821492\n",
      "\n",
      "\n",
      "3-Gram Model:\n",
      "\n",
      "\tCount= 1 -> Good Turing Count= 0.1348203459059289\n",
      "\tCount= 2 -> Good Turing Count= 0.8115976331360947\n",
      "\tCount= 3 -> Good Turing Count= 1.6727909011373578\n",
      "\tCount= 4 -> Good Turing Count= 2.688284518828452\n",
      "\tCount= 5 -> Good Turing Count= 3.0583657587548636\n",
      "\tCount= 6 -> Good Turing Count= 6.305343511450381\n",
      "\tCount= 7 -> Good Turing Count= 4.5423728813559325\n",
      "\tCount= 8 -> Good Turing Count= 6.7164179104477615\n",
      "\tCount= 9 -> Good Turing Count= 6.0\n",
      "\tCount= 10 -> Good Turing Count= 9.166666666666666\n",
      "\n",
      " Discounting value d for 3-gram= 1.390333987231656\n",
      "\n",
      "\n",
      "4-Gram Model:\n",
      "\n",
      "\tCount= 1 -> Good Turing Count= 0.046251818506217235\n",
      "\tCount= 2 -> Good Turing Count= 0.5839762611275965\n",
      "\tCount= 3 -> Good Turing Count= 1.2804878048780488\n",
      "\tCount= 4 -> Good Turing Count= 2.9047619047619047\n",
      "\tCount= 5 -> Good Turing Count= 1.6721311475409837\n",
      "\tCount= 6 -> Good Turing Count= 7.823529411764706\n",
      "\tCount= 7 -> Good Turing Count= 6.7368421052631575\n",
      "\tCount= 8 -> Good Turing Count= 3.9375\n",
      "\tCount= 9 -> Good Turing Count= 4.285714285714286\n",
      "\tCount= 10 -> Good Turing Count= 7.333333333333333\n",
      "\n",
      " Discounting value d for 4-gram= 1.8395471927109768\n",
      "\n"
     ]
    }
   ],
   "source": [
    "freq_count_ngram = {1:{},2:{},3:{},4:{}}\n",
    "dict_ngram_sorted_by_count = {1:{},2:{},3:{},4:{}}\n",
    "for i in range(1,5):\n",
    "    dict_ngram_sorted_by_count[i] = sorted(dict_wordseq_count[i].items(), key=lambda value: value[1], reverse=True)\n",
    "\n",
    "for i in range(1,5):\n",
    "    for k in dict_ngram_sorted_by_count[i]:\n",
    "        try:\n",
    "            freq_count_ngram[i][k[1]]+=1\n",
    "        except KeyError:\n",
    "            freq_count_ngram[i].update({k[1]:1})\n",
    "\n",
    "            \n",
    "# print(dict_ngram_sorted_by_count[2])\n",
    "# print(freq_count_ngram[1])\n",
    "\n",
    "good_turing_counts = {1:{},2:{},3:{},4:{}}\n",
    "# good_turing_prob = {1:{},2:{},3:{},4:{}}\n",
    "for i in range(1,5):\n",
    "    for k in freq_count_ngram[i]:\n",
    "        try:\n",
    "            c_new_gt = (k+1)*freq_count_ngram[i][k+1]/(freq_count_ngram[i][k])\n",
    "            good_turing_counts[i].update({k: c_new_gt})\n",
    "#             good_turing_prob[i].update({k: c_new_gt/(len(dict_ngram[i])) })\n",
    "        except KeyError:\n",
    "            good_turing_counts[i].update({k: 0})\n",
    "#             good_turing_prob[i].update({k: 0})\n",
    "    \n",
    "# print(good_turing_counts)\n",
    "d_good_turing = {}\n",
    "for k in range(1,5):\n",
    "    s = 0\n",
    "    print(\"\\n{}-Gram Model:\\n\".format(k))\n",
    "    for i in range(1,11):\n",
    "        print(\"\\tCount= {} -> Good Turing Count= {}\".format(i,good_turing_counts[k][i]))\n",
    "        s += i-good_turing_counts[k][i]\n",
    "    s = s/10\n",
    "    print(\"\\n Discounting value d for {}-gram= {}\\n\".format(k,s))\n",
    "    d_good_turing.update({k:s})  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Good Turing Probabilities for Bigrams using Discounting value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 0.32000756072204894,\n",
       " 2: 1.1139988186650915,\n",
       " 3: 1.9533404029692472,\n",
       " 4: 2.8718783930510314,\n",
       " 5: 4.378071833648393,\n",
       " 6: 4.461139896373057,\n",
       " 7: 6.861788617886178,\n",
       " 8: 6.5260663507109005,\n",
       " 9: 8.03921568627451,\n",
       " 10: 7.780487804878049,\n",
       " 11: 85.93059953651785,\n",
       " 12: 59.93059953651785,\n",
       " 13: 67.93059953651785,\n",
       " 14: 56.93059953651785,\n",
       " 15: 44.93059953651785,\n",
       " 16: 37.93059953651785,\n",
       " 17: 38.93059953651785,\n",
       " 18: 32.93059953651785,\n",
       " 19: 25.93059953651785,\n",
       " 20: 19.93059953651785,\n",
       " 21: 17.93059953651785,\n",
       " 22: 20.93059953651785,\n",
       " 23: 14.930599536517851,\n",
       " 24: 16.93059953651785,\n",
       " 25: 14.930599536517851,\n",
       " 26: 19.93059953651785,\n",
       " 27: 15.930599536517851,\n",
       " 28: 11.930599536517851,\n",
       " 29: 7.9305995365178505,\n",
       " 30: 9.930599536517851,\n",
       " 31: 4.9305995365178505,\n",
       " 32: 5.9305995365178505,\n",
       " 33: 9.930599536517851,\n",
       " 34: 4.9305995365178505,\n",
       " 35: 2.9305995365178505,\n",
       " 36: 2.9305995365178505,\n",
       " 37: 3.9305995365178505,\n",
       " 38: 3.9305995365178505,\n",
       " 39: 7.9305995365178505,\n",
       " 40: 1.9305995365178508,\n",
       " 41: 0.9305995365178508,\n",
       " 42: 1.9305995365178508,\n",
       " 43: 1.9305995365178508,\n",
       " 44: 10.930599536517851,\n",
       " 45: 3.9305995365178505,\n",
       " 46: 1.9305995365178508,\n",
       " 47: 3.9305995365178505,\n",
       " 48: 0.9305995365178508,\n",
       " 49: 1.9305995365178508,\n",
       " 50: 1.9305995365178508,\n",
       " 51: 0,\n",
       " 52: -0.06940046348214923,\n",
       " 53: 1.9305995365178508,\n",
       " 54: -0.06940046348214923,\n",
       " 55: 0.9305995365178508,\n",
       " 56: 0.9305995365178508,\n",
       " 57: 0.9305995365178508,\n",
       " 58: -0.06940046348214923,\n",
       " 59: 0.9305995365178508,\n",
       " 60: -0.06940046348214923,\n",
       " 61: 0,\n",
       " 62: 0.9305995365178508,\n",
       " 63: 2.9305995365178505,\n",
       " 64: 2.9305995365178505,\n",
       " 65: 0,\n",
       " 66: -0.06940046348214923,\n",
       " 67: 0,\n",
       " 68: -0.06940046348214923,\n",
       " 69: -0.06940046348214923,\n",
       " 70: -0.06940046348214923,\n",
       " 71: -0.06940046348214923,\n",
       " 72: 0.9305995365178508,\n",
       " 73: 0,\n",
       " 74: -0.06940046348214923,\n",
       " 75: -0.06940046348214923,\n",
       " 76: 1.9305995365178508,\n",
       " 77: 0.9305995365178508,\n",
       " 78: 0.9305995365178508,\n",
       " 79: 0.9305995365178508,\n",
       " 80: 1.9305995365178508,\n",
       " 81: 0,\n",
       " 82: 0,\n",
       " 83: -0.06940046348214923,\n",
       " 84: 0,\n",
       " 85: 0,\n",
       " 86: -0.06940046348214923,\n",
       " 87: 0,\n",
       " 88: 0.9305995365178508,\n",
       " 89: 0,\n",
       " 90: -0.06940046348214923,\n",
       " 91: 1.9305995365178508,\n",
       " 92: -0.06940046348214923,\n",
       " 93: -0.06940046348214923,\n",
       " 94: -0.06940046348214923,\n",
       " 95: 0,\n",
       " 96: 1.9305995365178508,\n",
       " 97: 0,\n",
       " 98: 0.9305995365178508,\n",
       " 99: 0,\n",
       " 100: 0,\n",
       " 101: 0,\n",
       " 102: 0.9305995365178508,\n",
       " 103: 0,\n",
       " 104: 0,\n",
       " 105: -0.06940046348214923,\n",
       " 106: 0,\n",
       " 107: -0.06940046348214923,\n",
       " 108: -0.06940046348214923,\n",
       " 109: 0,\n",
       " 110: 0,\n",
       " 111: -0.06940046348214923,\n",
       " 112: -0.06940046348214923,\n",
       " 113: -0.06940046348214923,\n",
       " 114: 0.9305995365178508,\n",
       " 115: 0,\n",
       " 116: 0,\n",
       " 117: 0,\n",
       " 118: -0.06940046348214923,\n",
       " 119: 0,\n",
       " 120: -0.06940046348214923,\n",
       " 121: -0.06940046348214923,\n",
       " 122: 0,\n",
       " 123: 0,\n",
       " 124: 0,\n",
       " 125: 1.9305995365178508,\n",
       " 126: 0,\n",
       " 127: -0.06940046348214923,\n",
       " 128: 0,\n",
       " 129: 0,\n",
       " 130: -0.06940046348214923,\n",
       " 131: 0,\n",
       " 132: 0,\n",
       " 133: 0,\n",
       " 134: 0,\n",
       " 135: 0,\n",
       " 136: 0,\n",
       " 137: -0.06940046348214923,\n",
       " 138: -0.06940046348214923,\n",
       " 139: 0,\n",
       " 140: 0,\n",
       " 141: 0,\n",
       " 142: 0,\n",
       " 143: 0,\n",
       " 144: 0,\n",
       " 145: -0.06940046348214923,\n",
       " 146: 0,\n",
       " 147: -0.06940046348214923,\n",
       " 148: 0,\n",
       " 149: -0.06940046348214923,\n",
       " 150: 0,\n",
       " 151: 0,\n",
       " 152: 0,\n",
       " 153: 0,\n",
       " 154: 0,\n",
       " 155: 0,\n",
       " 156: 0,\n",
       " 157: 0,\n",
       " 158: -0.06940046348214923,\n",
       " 159: 0,\n",
       " 160: 0,\n",
       " 161: 0,\n",
       " 162: 0,\n",
       " 163: 0,\n",
       " 164: 0,\n",
       " 165: 0,\n",
       " 166: 0,\n",
       " 167: 0,\n",
       " 168: -0.06940046348214923,\n",
       " 169: 0,\n",
       " 170: -0.06940046348214923,\n",
       " 171: 0,\n",
       " 172: 0,\n",
       " 173: -0.06940046348214923,\n",
       " 174: 0,\n",
       " 175: 0,\n",
       " 176: 0,\n",
       " 177: 0,\n",
       " 178: 0,\n",
       " 179: 0,\n",
       " 180: 0,\n",
       " 181: 0,\n",
       " 182: 0,\n",
       " 183: 0,\n",
       " 184: 0,\n",
       " 185: -0.06940046348214923,\n",
       " 186: 0,\n",
       " 187: 0,\n",
       " 188: -0.06940046348214923,\n",
       " 189: 0,\n",
       " 190: 0,\n",
       " 191: 0,\n",
       " 192: 0,\n",
       " 193: 0,\n",
       " 194: 0,\n",
       " 195: 0,\n",
       " 196: 0,\n",
       " 197: 0,\n",
       " 198: 0,\n",
       " 199: 0,\n",
       " 200: 0,\n",
       " 201: 0,\n",
       " 202: 0,\n",
       " 203: 0,\n",
       " 204: 0,\n",
       " 205: 0,\n",
       " 206: 0,\n",
       " 207: 0,\n",
       " 208: -0.06940046348214923,\n",
       " 209: 0,\n",
       " 210: 0,\n",
       " 211: 0,\n",
       " 212: 0,\n",
       " 213: 0,\n",
       " 214: 0,\n",
       " 215: 0,\n",
       " 216: 0,\n",
       " 217: 0,\n",
       " 218: 0,\n",
       " 219: -0.06940046348214923,\n",
       " 220: 0,\n",
       " 221: 0,\n",
       " 222: 0,\n",
       " 223: 0,\n",
       " 224: 0,\n",
       " 225: 0,\n",
       " 226: 0,\n",
       " 227: 0,\n",
       " 228: 0,\n",
       " 229: 0,\n",
       " 230: 0,\n",
       " 231: 0,\n",
       " 232: 0,\n",
       " 233: 0,\n",
       " 234: 0,\n",
       " 235: -0.06940046348214923,\n",
       " 236: 0,\n",
       " 237: 0,\n",
       " 238: 0,\n",
       " 239: 0,\n",
       " 240: 0,\n",
       " 241: 0,\n",
       " 242: 0,\n",
       " 243: 0,\n",
       " 244: 0,\n",
       " 245: 0,\n",
       " 246: 0,\n",
       " 247: 0,\n",
       " 248: 0,\n",
       " 249: 0,\n",
       " 250: 0,\n",
       " 251: 0,\n",
       " 252: 0,\n",
       " 253: 0,\n",
       " 254: 0,\n",
       " 255: 0,\n",
       " 256: 0,\n",
       " 257: -0.06940046348214923,\n",
       " 258: 0,\n",
       " 259: 0,\n",
       " 260: 0,\n",
       " 261: 0,\n",
       " 262: 0,\n",
       " 263: 0,\n",
       " 264: 0,\n",
       " 265: 0,\n",
       " 266: 0,\n",
       " 267: 0,\n",
       " 268: 0,\n",
       " 269: -0.06940046348214923,\n",
       " 270: 0,\n",
       " 271: 0,\n",
       " 272: 0,\n",
       " 273: 0,\n",
       " 274: 0,\n",
       " 275: 0,\n",
       " 276: 0,\n",
       " 277: 0,\n",
       " 278: 0,\n",
       " 279: 0,\n",
       " 280: -0.06940046348214923,\n",
       " 281: 0,\n",
       " 282: 0,\n",
       " 283: -0.06940046348214923,\n",
       " 284: 0,\n",
       " 285: 0,\n",
       " 286: 0,\n",
       " 287: 0,\n",
       " 288: 0,\n",
       " 289: 0,\n",
       " 290: 0,\n",
       " 291: 0,\n",
       " 292: 0,\n",
       " 293: 0,\n",
       " 294: 0,\n",
       " 295: 0,\n",
       " 296: 0,\n",
       " 297: 0,\n",
       " 298: 0,\n",
       " 299: 0,\n",
       " 300: 0,\n",
       " 301: 0,\n",
       " 302: 0,\n",
       " 303: 0,\n",
       " 304: 0,\n",
       " 305: 0,\n",
       " 306: 0,\n",
       " 307: 0,\n",
       " 308: 0,\n",
       " 309: 0,\n",
       " 310: 0,\n",
       " 311: 0,\n",
       " 312: 0,\n",
       " 313: 0,\n",
       " 314: 0,\n",
       " 315: 0,\n",
       " 316: 0,\n",
       " 317: 0,\n",
       " 318: 0,\n",
       " 319: 0,\n",
       " 320: 0,\n",
       " 321: 0,\n",
       " 322: 0,\n",
       " 323: 0,\n",
       " 324: 0,\n",
       " 325: 0,\n",
       " 326: 0,\n",
       " 327: 0,\n",
       " 328: 0,\n",
       " 329: 0,\n",
       " 330: 0,\n",
       " 331: 0,\n",
       " 332: 0,\n",
       " 333: 0,\n",
       " 334: 0,\n",
       " 335: 0,\n",
       " 336: 0,\n",
       " 337: 0,\n",
       " 338: 0,\n",
       " 339: 0,\n",
       " 340: 0,\n",
       " 341: 0,\n",
       " 342: 0,\n",
       " 343: 0,\n",
       " 344: 0,\n",
       " 345: 0,\n",
       " 346: 0,\n",
       " 347: 0,\n",
       " 348: 0,\n",
       " 349: 0,\n",
       " 350: 0,\n",
       " 351: 0,\n",
       " 352: 0,\n",
       " 353: 0,\n",
       " 354: 0,\n",
       " 355: 0,\n",
       " 356: 0,\n",
       " 357: 0,\n",
       " 358: 0,\n",
       " 359: 0,\n",
       " 360: 0,\n",
       " 361: 0,\n",
       " 362: -0.06940046348214923,\n",
       " 363: 0,\n",
       " 364: 0,\n",
       " 365: 0,\n",
       " 366: 0,\n",
       " 367: 0,\n",
       " 368: 0,\n",
       " 369: 0,\n",
       " 370: 0,\n",
       " 371: 0,\n",
       " 372: 0,\n",
       " 373: 0,\n",
       " 374: 0,\n",
       " 375: 0,\n",
       " 376: 0,\n",
       " 377: 0,\n",
       " 378: 0,\n",
       " 379: 0,\n",
       " 380: 0,\n",
       " 381: 0,\n",
       " 382: 0,\n",
       " 383: 0,\n",
       " 384: 0,\n",
       " 385: 0,\n",
       " 386: 0,\n",
       " 387: 0,\n",
       " 388: 0,\n",
       " 389: 0,\n",
       " 390: 0,\n",
       " 391: 0,\n",
       " 392: 0,\n",
       " 393: -0.06940046348214923,\n",
       " 394: 0,\n",
       " 395: 0,\n",
       " 396: 0,\n",
       " 397: 0,\n",
       " 398: 0,\n",
       " 399: 0,\n",
       " 400: 0,\n",
       " 401: 0,\n",
       " 402: 0,\n",
       " 403: 0,\n",
       " 404: 0,\n",
       " 405: 0,\n",
       " 406: 0,\n",
       " 407: 0,\n",
       " 408: 0,\n",
       " 409: 0,\n",
       " 410: 0,\n",
       " 411: 0,\n",
       " 412: 0,\n",
       " 413: 0,\n",
       " 414: 0,\n",
       " 415: 0,\n",
       " 416: 0,\n",
       " 417: 0,\n",
       " 418: 0,\n",
       " 419: 0,\n",
       " 420: 0,\n",
       " 421: 0,\n",
       " 422: 0,\n",
       " 423: 0,\n",
       " 424: 0,\n",
       " 425: 0,\n",
       " 426: 0,\n",
       " 427: 0,\n",
       " 428: 0,\n",
       " 429: 0,\n",
       " 430: 0,\n",
       " 431: 0,\n",
       " 432: 0,\n",
       " 433: 0,\n",
       " 434: 0,\n",
       " 435: 0,\n",
       " 436: 0,\n",
       " 437: 0,\n",
       " 438: 0,\n",
       " 439: 0,\n",
       " 440: 0,\n",
       " 441: 0,\n",
       " 442: 0,\n",
       " 443: 0,\n",
       " 444: 0,\n",
       " 445: 0,\n",
       " 446: 0,\n",
       " 447: 0,\n",
       " 448: 0,\n",
       " 449: 0,\n",
       " 450: 0,\n",
       " 451: 0,\n",
       " 452: 0,\n",
       " 453: 0,\n",
       " 454: 0,\n",
       " 455: 0,\n",
       " 456: 0,\n",
       " 457: 0,\n",
       " 458: 0,\n",
       " 459: 0,\n",
       " 460: 0,\n",
       " 461: 0,\n",
       " 462: 0,\n",
       " 463: 0,\n",
       " 464: 0,\n",
       " 465: 0,\n",
       " 466: 0,\n",
       " 467: 0,\n",
       " 468: 0,\n",
       " 469: 0,\n",
       " 470: 0,\n",
       " 471: 0,\n",
       " 472: 0,\n",
       " 473: 0,\n",
       " 474: 0,\n",
       " 475: 0,\n",
       " 476: 0,\n",
       " 477: 0,\n",
       " 478: 0,\n",
       " 479: 0,\n",
       " 480: 0,\n",
       " 481: 0,\n",
       " 482: 0,\n",
       " 483: 0,\n",
       " 484: 0,\n",
       " 485: 0,\n",
       " 486: 0,\n",
       " 487: 0,\n",
       " 488: 0,\n",
       " 489: 0,\n",
       " 490: 0,\n",
       " 491: 0,\n",
       " 492: 0,\n",
       " 493: 0,\n",
       " 494: 0,\n",
       " 495: 0,\n",
       " 496: 0,\n",
       " 497: 0,\n",
       " 498: 0,\n",
       " 499: 0,\n",
       " 500: 0,\n",
       " 501: 0,\n",
       " 502: 0,\n",
       " 503: 0,\n",
       " 504: 0,\n",
       " 505: 0,\n",
       " 506: 0,\n",
       " 507: 0,\n",
       " 508: 0,\n",
       " 509: 0,\n",
       " 510: 0,\n",
       " 511: 0,\n",
       " 512: 0,\n",
       " 513: 0,\n",
       " 514: 0,\n",
       " 515: 0,\n",
       " 516: 0,\n",
       " 517: 0,\n",
       " 518: 0,\n",
       " 519: 0,\n",
       " 520: 0,\n",
       " 521: 0,\n",
       " 522: 0,\n",
       " 523: 0,\n",
       " 524: 0,\n",
       " 525: 0,\n",
       " 526: 0,\n",
       " 527: 0,\n",
       " 528: 0,\n",
       " 529: 0,\n",
       " 530: 0,\n",
       " 531: 0,\n",
       " 532: 0,\n",
       " 533: 0,\n",
       " 534: 0,\n",
       " 535: 0,\n",
       " 536: 0,\n",
       " 537: 0,\n",
       " 538: 0,\n",
       " 539: 0,\n",
       " 540: 0,\n",
       " 541: 0,\n",
       " 542: 0,\n",
       " 543: 0,\n",
       " 544: 0,\n",
       " 545: 0,\n",
       " 546: 0,\n",
       " 547: 0,\n",
       " 548: 0,\n",
       " 549: 0,\n",
       " 550: 0,\n",
       " 551: 0,\n",
       " 552: 0,\n",
       " 553: 0,\n",
       " 554: 0,\n",
       " 555: 0,\n",
       " 556: 0,\n",
       " 557: 0,\n",
       " 558: 0,\n",
       " 559: 0,\n",
       " 560: 0,\n",
       " 561: 0,\n",
       " 562: 0,\n",
       " 563: 0,\n",
       " 564: 0,\n",
       " 565: 0,\n",
       " 566: 0,\n",
       " 567: 0,\n",
       " 568: 0,\n",
       " 569: 0,\n",
       " 570: 0,\n",
       " 571: 0,\n",
       " 572: 0,\n",
       " 573: 0,\n",
       " 574: 0,\n",
       " 575: 0,\n",
       " 576: 0,\n",
       " 577: 0,\n",
       " 578: 0,\n",
       " 579: 0,\n",
       " 580: 0,\n",
       " 581: -0.06940046348214923,\n",
       " 582: 0,\n",
       " 583: 0,\n",
       " 584: 0,\n",
       " 585: 0,\n",
       " 586: 0,\n",
       " 587: 0,\n",
       " 588: 0,\n",
       " 589: 0,\n",
       " 590: 0,\n",
       " 591: 0,\n",
       " 592: 0,\n",
       " 593: 0,\n",
       " 594: 0,\n",
       " 595: 0,\n",
       " 596: 0,\n",
       " 597: 0,\n",
       " 598: 0,\n",
       " 599: 0,\n",
       " 600: 0,\n",
       " 601: 0,\n",
       " 602: 0,\n",
       " 603: 0,\n",
       " 604: 0,\n",
       " 605: 0,\n",
       " 606: 0,\n",
       " 607: 0,\n",
       " 608: 0,\n",
       " 609: 0,\n",
       " 610: 0,\n",
       " 611: 0,\n",
       " 612: 0,\n",
       " 613: 0,\n",
       " 614: 0,\n",
       " 615: 0,\n",
       " 616: 0,\n",
       " 617: 0,\n",
       " 618: 0,\n",
       " 619: 0,\n",
       " 620: 0,\n",
       " 621: 0,\n",
       " 622: 0,\n",
       " 623: 0,\n",
       " 624: 0,\n",
       " 625: 0,\n",
       " 626: 0,\n",
       " 627: 0,\n",
       " 628: 0,\n",
       " 629: 0,\n",
       " 630: 0,\n",
       " 631: 0,\n",
       " 632: 0,\n",
       " 633: 0,\n",
       " 634: 0,\n",
       " 635: 0,\n",
       " 636: 0,\n",
       " 637: 0,\n",
       " 638: 0,\n",
       " 639: 0,\n",
       " 640: 0,\n",
       " 641: 0,\n",
       " 642: 0,\n",
       " 643: 0,\n",
       " 644: 0,\n",
       " 645: 0,\n",
       " 646: 0,\n",
       " 647: 0,\n",
       " 648: 0,\n",
       " 649: 0,\n",
       " 650: 0,\n",
       " 651: 0,\n",
       " 652: 0,\n",
       " 653: 0,\n",
       " 654: 0,\n",
       " 655: 0,\n",
       " 656: 0,\n",
       " 657: 0,\n",
       " 658: 0,\n",
       " 659: 0,\n",
       " 660: 0,\n",
       " 661: 0,\n",
       " 662: 0,\n",
       " 663: 0,\n",
       " 664: 0,\n",
       " 665: 0,\n",
       " 666: 0,\n",
       " 667: 0,\n",
       " 668: 0,\n",
       " 669: 0,\n",
       " 670: 0,\n",
       " 671: 0,\n",
       " 672: 0,\n",
       " 673: 0,\n",
       " 674: 0,\n",
       " 675: 0,\n",
       " 676: 0,\n",
       " 677: 0,\n",
       " 678: 0,\n",
       " 679: 0,\n",
       " 680: 0,\n",
       " 681: 0,\n",
       " 682: 0,\n",
       " 683: 0,\n",
       " 684: 0,\n",
       " 685: 0,\n",
       " 686: 0,\n",
       " 687: 0,\n",
       " 688: 0,\n",
       " 689: 0,\n",
       " 690: 0,\n",
       " 691: 0,\n",
       " 692: 0,\n",
       " 693: 0,\n",
       " 694: 0,\n",
       " 695: 0,\n",
       " 696: 0,\n",
       " 697: 0,\n",
       " 698: 0,\n",
       " 699: 0,\n",
       " 700: 0,\n",
       " 701: 0,\n",
       " 702: 0,\n",
       " 703: 0,\n",
       " 704: 0,\n",
       " 705: 0,\n",
       " 706: 0,\n",
       " 707: 0,\n",
       " 708: 0,\n",
       " 709: 0,\n",
       " 710: 0,\n",
       " 711: 0,\n",
       " 712: 0,\n",
       " 713: 0,\n",
       " 714: 0,\n",
       " 715: 0,\n",
       " 716: 0,\n",
       " 717: 0,\n",
       " 718: 0,\n",
       " 719: 0,\n",
       " 720: 0,\n",
       " 721: 0,\n",
       " 722: 0,\n",
       " 723: 0,\n",
       " 724: 0,\n",
       " 725: 0,\n",
       " 726: 0,\n",
       " 727: 0,\n",
       " 728: 0,\n",
       " 729: 0,\n",
       " 730: 0,\n",
       " 731: 0,\n",
       " 732: 0,\n",
       " 733: 0,\n",
       " 734: 0,\n",
       " 735: 0,\n",
       " 736: 0,\n",
       " 737: 0,\n",
       " 738: 0,\n",
       " 739: 0,\n",
       " 740: 0,\n",
       " 741: 0,\n",
       " 742: 0,\n",
       " 743: 0,\n",
       " 744: 0,\n",
       " 745: 0,\n",
       " 746: 0,\n",
       " 747: 0,\n",
       " 748: 0,\n",
       " 749: 0,\n",
       " 750: 0,\n",
       " 751: 0,\n",
       " 752: 0,\n",
       " 753: 0,\n",
       " 754: 0,\n",
       " 755: 0,\n",
       " 756: 0,\n",
       " 757: 0,\n",
       " 758: 0,\n",
       " 759: 0,\n",
       " 760: 0,\n",
       " 761: 0,\n",
       " 762: 0,\n",
       " 763: 0,\n",
       " 764: 0,\n",
       " 765: 0,\n",
       " 766: 0}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = d_good_turing[2]\n",
    "good_turing_count_d_bigram = {}\n",
    "for k in range(1,11):\n",
    "    good_turing_count_d_bigram.update({k:good_turing_counts[2][k]})\n",
    "\n",
    "for k in range(11,max(freq_count_ngram[2].keys())):\n",
    "    try:\n",
    "        good_turing_count_d_bigram.update({k:freq_count_ngram[2][k]-d})\n",
    "    except KeyError:\n",
    "        good_turing_count_d_bigram.update({k:0})\n",
    "\n",
    "#         if freq_count_ngram[2][k]>d:\n",
    "#             prob_gt_bigram.update({k: (freq_count_ngram[2][k]-d)/(len(dict_ngram[2]))})\n",
    "#         else:\n",
    "#             prob_gt_bigram.update({k: (freq_count_ngram[2][1])/(len(dict_ngram[2]))})\n",
    "#     except KeyError:\n",
    "#         prob_gt_bigram.update({k: (freq_count_ngram[2][1])/(len(dict_ngram[2]))})\n",
    "\n",
    "good_turing_count_d_bigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre processing to find Perplexity for Bigram with Good Turing Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=2\n",
    "list_bigram_testing = []\n",
    "for w in words_sent_corpus_testing:\n",
    "    for w2 in list(ngrams(w,n)):\n",
    "        if n==1:\n",
    "            list_ngram_testing.append((w2))\n",
    "        else:\n",
    "            if w2!=('','','','') and w2!=('','','') and w2!=('','') and w2!=(''):\n",
    "                list_bigram_testing.append(w2)\n",
    "\n",
    "                \n",
    "dict_bigram_count_testing={} #counts of bigrams in testing portion of corpus     \n",
    "if len(dict_bigram_count_testing)!=0: # to ensure this function isn't executed twice in the notebook\n",
    "    pass\n",
    "else:\n",
    "    for i in list_bigram_testing:\n",
    "        try:\n",
    "            dict_bigram_count_testing[i]+=1\n",
    "        except KeyError:\n",
    "            dict_bigram_count_testing.update({i:1})\n",
    "\n",
    "# dict_bigram_count_testing\n",
    "# list_bigram_testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perplexity for Bigram after Add-1 Smoothing and Good Turing Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding bigrams in testing\n",
    "list_unique_bigram_testing=[]\n",
    "list_words=[]\n",
    "for sent in words_sent_corpus_testing:\n",
    "    l_temp=[]\n",
    "    for i in range(len(sent)-1):\n",
    "        if (sent[i],sent[i+1]) not in list_unique_bigram_testing:\n",
    "            list_unique_bigram_testing.append((sent[i],sent[i+1]))\n",
    "        l_temp.append(sent[i])\n",
    "    list_words.append(l_temp)\n",
    "\n",
    "c = 0 # total no of bigrams\n",
    "c2 = 0\n",
    "perp = 0\n",
    "perp_gt = 0\n",
    "for s in list_words:\n",
    "    for i in range(len(s)-1):\n",
    "        c += 1\n",
    "        p = add1_bigram_prob(tuple([s[i],s[i+1]]))\n",
    "        try:\n",
    "            pgt = prob_gt_bigram[dict_bigram_count_testing[tuple([s[i],s[i+1]])]]\n",
    "        except KeyError:\n",
    "            continue\n",
    "        perp += math.log(p)\n",
    "        perp_gt += math.log(pgt)\n",
    "\n",
    "# print(perp_t)\n",
    "# print('len(list_bigrams_testing):',len(list_bigrams_testing))\n",
    "# print('len(list_words):',len(list_words))\n",
    "perp = perp*(-1)/c\n",
    "perp = math.exp(perp)\n",
    "\n",
    "perp_gt = perp_gt*(-1)/c\n",
    "perp_gt = math.exp(perp_gt)\n",
    "\n",
    "print(\"Perplexity for Bigram with Add-1 smoothing is:{}\".format(round(perp,2)))\n",
    "print(\"Perplexity for Bigram with Good Turing smoothing is:{}\".format(round(perp_gt,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
