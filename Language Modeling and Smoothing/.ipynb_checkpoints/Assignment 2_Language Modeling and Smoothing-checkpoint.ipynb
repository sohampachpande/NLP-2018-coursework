{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.util import ngrams\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assistance functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to find n Choose r\n",
    "def nCr(n,r):\n",
    "    r2 = max(r,n-r)\n",
    "    ncr = 1\n",
    "    for m in range(n,r2,-1):\n",
    "        ncr = ncr*m\n",
    "    for m in range(1,n-r2+1):\n",
    "        ncr = ncr//m\n",
    "    return ncr\n",
    "\n",
    "\n",
    "# split sentence\n",
    "def sent_splitter(sent, add_tag=1):\n",
    "    if type(sent)==list or type(sent)==tuple:\n",
    "        return sent\n",
    "    elif type(sent)==str:\n",
    "        punctuation_char = string.punctuation + '“”'\n",
    "        sent = sent.translate(str.maketrans('','',punctuation_char))\n",
    "        if add_tag:\n",
    "            sent = '<s> '+ sent.lower()+ ' </s>'\n",
    "        sent = sent.split(sep=' ')\n",
    "        return sent\n",
    "    else:\n",
    "        print(sent)\n",
    "        print(\"Invalid data passed to sentence splitter\")\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing of Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('theAdventuresofSherlockHolmes.txt',encoding=\"utf8\")\n",
    "# f = open('AliceAdventuresinWonderland.txt',encoding=\"utf8\")\n",
    "\n",
    "corpus = f.read()\n",
    "corpus = corpus.replace(\"'\", ' ')\n",
    "corpus = corpus.replace(\"-\", ' ')\n",
    "# Removing unneccessay punctuation characters except .,?,!\n",
    "punctuation_char = string.punctuation + '“”'\n",
    "punctuation_char = punctuation_char.replace('!','')\n",
    "punctuation_char = punctuation_char.replace('?','')\n",
    "punctuation_char = punctuation_char.replace('.','')\n",
    "corpus = corpus.translate(str.maketrans('','',punctuation_char))\n",
    "corpus = corpus.replace('--', '')\n",
    "# Some unneccessary Roman Numericals\n",
    "corpus = corpus.replace('\\nI.', '')\n",
    "corpus = corpus.replace('\\nII.', '')\n",
    "corpus = corpus.replace('\\nIII.', '')\n",
    "# Removing new line characters\n",
    "corpus = corpus.replace('\\n', ' ')\n",
    "\n",
    "# Splitting Corpus into Sentences\n",
    "sent_corpus = nltk.tokenize.sent_tokenize(corpus)\n",
    "\n",
    "# Adding start and end of sentence tags\n",
    "for i in range(len(sent_corpus)):\n",
    "    sent_corpus[i] = '<s> '+ sent_corpus[i].replace('.','').lower()+ ' </s>'     \n",
    "\n",
    "# Splitting Sentences into words\n",
    "words_sent_corpus = []\n",
    "# count_words = 0\n",
    "for i in range(len(sent_corpus)):\n",
    "    words_sent_corpus.append(sent_corpus[i].split(sep=' '))\n",
    "#     count_words += len(sent_corpus[i].split(sep=' '))-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spliting Corpus 80:20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = len(words_sent_corpus)\n",
    "words_sent_corpus_testing = words_sent_corpus[math.ceil(0.8*l):]\n",
    "words_sent_corpus = words_sent_corpus[:math.ceil(0.8*l)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding n-grams and their counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_ngram(n):\n",
    "    list_ngram = []\n",
    "    for w in words_sent_corpus:\n",
    "        for w2 in list(ngrams(w,n)):\n",
    "            if n==1:\n",
    "                list_ngram.append((w2))\n",
    "            else:\n",
    "                if w2!=('','','','') and w2!=('','','') and w2!=('','') and w2!=(''):\n",
    "                    list_ngram.append(w2)\n",
    "    return list_ngram\n",
    "\n",
    "dict_ngram={} # key= n, value= ngram\n",
    "for i in range(1,5):\n",
    "    dict_ngram.update({i:find_ngram(i)})\n",
    "\n",
    "# Count of k word sequence's for k = 1,2,3,4 for kth order MLE\n",
    "# 1-unigram,2-bigram,3-trigram,4-quadgram\n",
    "dict_wordseq_count = {1:{},2:{},3:{},4:{}}  \n",
    "def form_count_for_MLE(n):\n",
    "    global dict_wordseq_count\n",
    "    if len(dict_wordseq_count[n])!=0: # to ensure this function isn't executed twice in the notebook\n",
    "        pass\n",
    "    else:\n",
    "        for i in dict_ngram[n]:\n",
    "            try:\n",
    "                dict_wordseq_count[n][i]+=1\n",
    "            except KeyError:\n",
    "                dict_wordseq_count[n].update({i:1})\n",
    "for i in range(1,5):\n",
    "    form_count_for_MLE(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of ngrams present and number of ngrams possible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of 1-gram present (counting repetitions): 94997\n",
      "Number of Unique 1-gram: 7598\n",
      "Number of 1-gram possible: 7598\n",
      "\n",
      "Number of 2-gram present (counting repetitions): 89368\n",
      "Number of Unique 2-gram: 42124\n",
      "Number of 2-gram possible: 887194626\n",
      "\n",
      "Number of 3-gram present (counting repetitions): 83961\n",
      "Number of Unique 3-gram: 69349\n",
      "Number of 3-gram possible: 55584099100474\n",
      "\n",
      "Number of 4-gram present (counting repetitions): 78540\n",
      "Number of Unique 4-gram: 75112\n",
      "Number of 4-gram possible: 1326146093198017070\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,5):\n",
    "    print(\"Number of {}-gram present (counting repetitions): {}\".format(str(i),len(dict_ngram[i])))\n",
    "    print(\"Number of Unique {}-gram: {}\".format(str(i),len(dict_wordseq_count[i])))\n",
    "    print(\"Number of {}-gram possible: {}\\n\".format(str(i),nCr(len(dict_wordseq_count[i]),i)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum likelihood Estimate for ngram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Function to find probabity of given word sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Sentences with Probability:\n",
      "\n",
      "Sentence: visit\n",
      "\n",
      "1-Gram Model, Probability:9.473983388949125e-05\n",
      "\n",
      "Sentence: i sat\n",
      "\n",
      "2-Gram Model, Probability:0.0022271714922048997\n",
      "\n",
      "Sentence: do you mean\n",
      "\n",
      "3-Gram Model, Probability:0.045454545454545456\n",
      "\n",
      "Sentence: i had got when\n",
      "\n",
      "4-Gram Model, Probability:1.0\n"
     ]
    }
   ],
   "source": [
    "def mle_p_ngram(n,wlist):\n",
    "#     if type(wlist)==str:\n",
    "    wlist = sent_splitter(wlist,add_tag=0)\n",
    "        \n",
    "    if len(wlist)!=n:\n",
    "        print(\"Invalid model, number of words is not equal to {}\".format(n))\n",
    "        return -1\n",
    "    try:  \n",
    "        if n==1:\n",
    "            return float(dict_wordseq_count[n][(wlist[0],)])/(len(dict_ngram[n]))\n",
    "        elif n==2:\n",
    "            return float(dict_wordseq_count[n][tuple(wlist)])/dict_wordseq_count[n-1][(wlist[0],)]\n",
    "        elif n==3:\n",
    "            return float(dict_wordseq_count[n][tuple(wlist)])/dict_wordseq_count[n-1][wlist[0],wlist[1]]\n",
    "        elif n==4:\n",
    "            return float(dict_wordseq_count[n][tuple(wlist)])/dict_wordseq_count[n-1][wlist[0],wlist[1],wlist[2]]\n",
    "    except KeyError: #Word not found\n",
    "        return 0\n",
    "\n",
    "n=2\n",
    "w = 'with an'\n",
    "# mle_p_ngram(n,w)\n",
    "\n",
    "print(\"Example Sentences with Probability:\")\n",
    "sl = ['visit','i sat','do you mean','i had got when']\n",
    "for i in range(4):\n",
    "    print('\\nSentence: {}\\n'.format(sl[i]))\n",
    "    try:\n",
    "        print(\"{}-Gram Model, Probability:{}\".format(i+1,mle_p_ngram(i+1,sl[i])))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find probability for all ngrams present in corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mle_word_dict = {1:{},2:{},3:{},4:{}}\n",
    "for n in range(1,5):\n",
    "    for w in dict_wordseq_count[n]:\n",
    "            try:\n",
    "                mle_word_dict[n][w] += mle_p_ngram(n,w)\n",
    "            except KeyError:\n",
    "                mle_word_dict[n].update({w:mle_p_ngram(n,w)})\n",
    "                    \n",
    "mle_wordseq_prob_list = {1:[],2:[],3:[],4:[]}\n",
    "mle_wordseq_list = {1:[],2:[],3:[],4:[]}\n",
    "\n",
    "for n in range(1,5):\n",
    "    for w in mle_word_dict[n]:\n",
    "        mle_wordseq_prob_list[n].append(mle_word_dict[n][w])\n",
    "        mle_wordseq_list[n].append(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to find probability of sentence for given ngram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Sentences with Probability:\n",
      "\n",
      "\n",
      "Sentence: i sat down\n",
      "\n",
      "1-Gram Model, Probability:6.501378758699384e-11\n",
      "2-Gram Model, Probability:2.9039371565473137e-06\n",
      "3-Gram Model, Probability:7.450176941702365e-05\n",
      "\n",
      "Sentence: do you mean that\n",
      "\n",
      "1-Gram Model, Probability:1.4668322521718687e-13\n",
      "2-Gram Model, Probability:9.5934619118052e-09\n",
      "3-Gram Model, Probability:0.0\n",
      "\n",
      "Sentence: the lamps had been lit but the blinds had not been drawn\n",
      "\n",
      "1-Gram Model, Probability:2.524209073501141e-36\n",
      "2-Gram Model, Probability:0.0\n",
      "3-Gram Model, Probability:2.0156754402622197e-10\n",
      "\n",
      "Sentence: so far i had got when we went to visit the scene of action\n",
      "\n",
      "1-Gram Model, Probability:4.4761176669234027e-41\n",
      "2-Gram Model, Probability:1.0186416757951622e-25\n",
      "3-Gram Model, Probability:4.16666666666667e-08\n"
     ]
    }
   ],
   "source": [
    "def probability_sentence(sent,model_no):\n",
    "    sent=sent_splitter(sent,add_tag=1)\n",
    "    prob = 0\n",
    "    if model_no==1:\n",
    "        l = -len(sent)\n",
    "    else:\n",
    "        l = model_no\n",
    "    for w in sent[:-l+1]:\n",
    "        index = sent.index(w)\n",
    "        if index+model_no<=len(sent):\n",
    "            temp = mle_p_ngram(model_no,sent[index:index+model_no])\n",
    "#             print(temp)\n",
    "            if temp==0: # How to take log\n",
    "                probt = float(\"-inf\")\n",
    "            else:\n",
    "                probt = math.log(temp)\n",
    "            prob += probt\n",
    "#             print (prob)\n",
    "    return math.exp(prob)\n",
    "\n",
    "\n",
    "print(\"Example Sentences with Probability:\\n\")\n",
    "sl = ['i sat down','do you mean that','the lamps had been lit but the blinds had not been drawn','so far i had got when we went to visit the scene of action']\n",
    "for s in sl:\n",
    "    print('\\nSentence: {}\\n'.format(s))\n",
    "    for i in range(1,4):\n",
    "        print(\"{}-Gram Model, Probability:{}\".format(i,probability_sentence(s,i)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Sentences generated using NGram Model:\n",
      "\n",
      "\n",
      "1-Gram Model\n",
      "\n",
      "<s> evil kent concern coldly perhaps? steel remarkably poured charming pile patient gleam warm! hunted wreath was consult masses career towards unpack outskirts 4d talked mousseline pacing\n",
      "<s> green glove limbs bowls insisted assistance? shrieked monomaniac b motion paradol specialist cubic wall papers hands skill eight dangling tiger language silently tempted gaol proves camera\n",
      "<s> wallenstein experience down altar humbler reconsider insists isle lights explained fuller army supplied be? sleepers maddening poor elements occupied attempts explain grate recognising thinks puckered nose?\n",
      "\n",
      "2-Gram Model\n",
      "\n",
      "<s> away after half past belief that her photograph? </s>\n",
      "<s> put on our investigation in foolscap and saw at </s>\n",
      "<s> many inquiries as none more one dreadful hours i remain dear watson if both glove and indicated a nod he hardly take effect which shows that something to claim to encamp upon bohemian soul </s>\n",
      "\n",
      "3-Gram Model\n",
      "\n",
      "<s> kindly hand me over my notes and figures </s>\n",
      "<s> quite an interesting study mr windibank asking him whether he was seized and searched </s>\n",
      "<s> said the police regulations he pretends to a b and c   the strong probability  is mr jabez wilson  said i laughing but since we know something of his little brain attic stocked with all its disadvantages to my face lengthened at this but the other papers were burned by your uncle of the amateur mendicant society who held a luxurious club in the working of it </s>\n",
      "\n",
      "4-Gram Model\n",
      "\n",
      "<s> an accident i presume? </s>\n",
      "<s> they come they go they come again  of course stands for  gesellschaft  which is the german who is so uncourteous to his verbs </s>\n",
      "<s> local aid is always either worthless or else biassed </s>\n"
     ]
    }
   ],
   "source": [
    "def prob_ngrams_starting_with_w(w_seq, model_no):\n",
    "    prob_list = []\n",
    "    word_list = []\n",
    "    indexing_list = []\n",
    "    if type(w_seq)==str:\n",
    "        w_seq= sent_splitter(w_seq, add_tag=0)\n",
    "#     print('prob_ngrams_w',w_seq)\n",
    "    for i in range(len(mle_wordseq_list[model_no])):\n",
    "        if model_no in [2,3,4]:\n",
    "            if mle_wordseq_list[model_no][i][:model_no-1] == tuple(w_seq):\n",
    "                indexing_list.append(len(word_list))\n",
    "                word_list.append(mle_wordseq_list[model_no][i])\n",
    "                prob_list.append(mle_wordseq_prob_list[model_no][i])\n",
    "        elif model_no==1:\n",
    "            indexing_list.append(len(word_list))\n",
    "            word_list.append(mle_wordseq_list[model_no][i])\n",
    "            prob_list.append(mle_wordseq_prob_list[model_no][i])            \n",
    "    return word_list, prob_list, indexing_list\n",
    "\n",
    "def sent_generate(model_no):\n",
    "    w1 = '<s>'\n",
    "    we = '</s>'\n",
    "    sfinal = '<s>'\n",
    "    if model_no==1:\n",
    "        w_seq = '<s>'\n",
    "        count = 0\n",
    "        while w_seq != '</s>':\n",
    "            word_list,prob_list,indexing_list = prob_ngrams_starting_with_w(w_seq,model_no)\n",
    "            next_w_index = np.random.choice(indexing_list,1,prob_list)\n",
    "            w_seq = word_list[int(next_w_index)][0]\n",
    "            sfinal += ' '+ w_seq\n",
    "            if count == 25:\n",
    "                break\n",
    "            count +=1\n",
    "        return sfinal\n",
    "    \n",
    "    elif model_no==2:\n",
    "        w_seq = '<s>'\n",
    "        while w_seq != '</s>':    \n",
    "            word_list,prob_list,indexing_list = prob_ngrams_starting_with_w((w_seq,),model_no)\n",
    "            next_w_index = np.random.choice(indexing_list,1,prob_list)\n",
    "            w_seq = word_list[int(next_w_index)][1]\n",
    "            sfinal += ' '+ w_seq\n",
    "        return sfinal\n",
    "    elif model_no==3:\n",
    "        w_seq = ['<s>']\n",
    "        # First get second word from bigram counts. \n",
    "        # This is valid as count('<s>','<s>',w)=count('<s>',w) in our case as we are not taking multiple '<s>'\n",
    "        word_list,prob_list,indexing_list = prob_ngrams_starting_with_w((w_seq[0],),model_no-1)\n",
    "        next_w_index = np.random.choice(indexing_list,1,prob_list)\n",
    "        w_seq.append(word_list[int(next_w_index)][1])\n",
    "#         print(w_seq)\n",
    "        sfinal += ' '+ word_list[int(next_w_index)][-1]\n",
    "        while w_seq[1] != '</s>':\n",
    "            word_list, prob_list, indexing_list = prob_ngrams_starting_with_w(w_seq,model_no)\n",
    "            next_w_index = np.random.choice(indexing_list,1,prob_list)\n",
    "            w_seq = word_list[int(next_w_index)][1:]\n",
    "#             print(w_seq)\n",
    "            sfinal += ' '+ w_seq[-1]\n",
    "        return sfinal\n",
    "    elif model_no==4:\n",
    "        w_seq = ['<s>']\n",
    "        # First get second word from bigram counts. \n",
    "        # This is valid as count('<s>','<s>',w)=count('<s>',w) in our case as we are not taking multiple '<s>'\n",
    "        # Similarly get third word from trigram\n",
    "        word_list,prob_list,indexing_list = prob_ngrams_starting_with_w((w_seq[0],),model_no-2)\n",
    "        next_w_index = np.random.choice(indexing_list,1,prob_list)\n",
    "        w_seq.append(word_list[int(next_w_index)][1])\n",
    "#         print(w_seq)\n",
    "        sfinal += ' '+ word_list[int(next_w_index)][-1]\n",
    "        \n",
    "        word_list, prob_list, indexing_list = prob_ngrams_starting_with_w(w_seq,model_no-1)\n",
    "        next_w_index = np.random.choice(indexing_list,1,prob_list)\n",
    "        w_seq = word_list[int(next_w_index)][:]\n",
    "#         print(w_seq)\n",
    "        sfinal += ' '+ word_list[int(next_w_index)][-1]\n",
    "        \n",
    "        while w_seq[2] != '</s>':\n",
    "            word_list, prob_list, indexing_list = prob_ngrams_starting_with_w(w_seq,model_no)\n",
    "            next_w_index = np.random.choice(indexing_list,1,prob_list)\n",
    "            w_seq = word_list[int(next_w_index)][1:]\n",
    "#             print(w_seq)\n",
    "            sfinal += ' '+ w_seq[-1]\n",
    "        return sfinal\n",
    "        \n",
    "print(\"Example Sentences generated using NGram Model:\\n\")\n",
    "for i in range(1,5):\n",
    "    print('\\n{}-Gram Model\\n'.format(i))\n",
    "    print(sent_generate(i))\n",
    "    print(sent_generate(i))\n",
    "    print(sent_generate(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add-1 smoothing for bigram + Updated count:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples with Drastic change in Counts:\n",
      "\n",
      "Bigram: ('of', 'the')\n",
      "Count: 581\n",
      "Post Add 1 Smoothing Count: 129.71034059527463\n",
      "Post Add 1 Smoothing Probability: 0.05952746241178276\n",
      "\n",
      "Bigram: ('it', 'is')\n",
      "Count: 269\n",
      "Post Add 1 Smoothing Count: 40.45317220543807\n",
      "Post Add 1 Smoothing Probability: 0.030211480362537766\n",
      "\n",
      "Bigram: ('that', 'i')\n",
      "Count: 185\n",
      "Post Add 1 Smoothing Count: 27.920805369127518\n",
      "Post Add 1 Smoothing Probability: 0.02080536912751678\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def add1_bigram_prob(wlist):\n",
    "#     if type(wlist)==str:\n",
    "    wlist=sent_splitter(wlist,add_tag=0)\n",
    "    if len(wlist)!=2:\n",
    "        print(\"Not a Bigram!\")\n",
    "        return -1\n",
    "    else:\n",
    "        try:\n",
    "            return (float(dict_wordseq_count[2][tuple(wlist)])+1)/(dict_wordseq_count[1][(wlist[0],)]+ len(dict_wordseq_count[1]))\n",
    "        except KeyError:\n",
    "            try:\n",
    "                return (1)/(dict_wordseq_count[1][(wlist[0],)]+ len(dict_wordseq_count[1]))\n",
    "            except KeyError:\n",
    "#               (\"The 1st word of bigram doesnt exist in training corpus\".format(wlist[0]))\n",
    "                return 1/(1+len(dict_wordseq_count[1]))\n",
    "def add1_bigram_count(wlist):\n",
    "    if type(wlist)==str:\n",
    "        wlist=sent_splitter(wlist,add_tag=0)\n",
    "    if len(wlist)!=2:\n",
    "        print(\"Not a Bigram!\")\n",
    "        return -1\n",
    "    else:\n",
    "        return dict_wordseq_count[1][(wlist[0],)]*add1_bigram_prob(wlist)\n",
    "    \n",
    "print(\"Examples with Drastic change in Counts:\\n\")\n",
    "wl = [('of','the'),('it', 'is'),('that', 'i')]\n",
    "for w in wl:\n",
    "    print(\"Bigram: {}\\nCount: {}\\nPost Add 1 Smoothing Count: {}\\nPost Add 1 Smoothing Probability: {}\\n\".format(w,dict_wordseq_count[2][tuple(w)],add1_bigram_count(w),add1_bigram_prob(w)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability of Sentence using Bigram with Add-1 smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Sentences with 2-gram model Probability after Add-1 Smoothing:\n",
      "\n",
      "Sentence: i sat down\n",
      "Probability:5.3189834149362046e-11\n",
      "\n",
      "Sentence: do you mean that\n",
      "Probability:5.9503724659307494e-15\n",
      "\n",
      "Sentence: the lamps had been lit but the blinds had not been drawn\n",
      "Probability:2.261365020126562e-40\n",
      "\n",
      "Sentence: so far i had got when we went to visit the scene of action\n",
      "Probability:1.9439558762275912e-47\n"
     ]
    }
   ],
   "source": [
    "def probability_sentence_add1_bigram(sent):\n",
    "    sent=sent_splitter(sent,add_tag=1)\n",
    "    prob = 0\n",
    "    for w in sent[:-1]:\n",
    "        index = sent.index(w)\n",
    "        if index+2<=len(sent):\n",
    "            try:\n",
    "                temp = add1_bigram_prob(sent[index:index+2])\n",
    "            except KeyError:\n",
    "                temp=0                \n",
    "#             print(temp)\n",
    "            if temp==0: # How to take log----take -inf\n",
    "                probt = float(\"-inf\")\n",
    "            else:\n",
    "                probt = math.log(temp)\n",
    "            prob += probt\n",
    "#             print (prob)\n",
    "    return math.exp(prob)\n",
    "\n",
    "\n",
    "print(\"Example Sentences with 2-gram model Probability after Add-1 Smoothing:\")\n",
    "sl = ['i sat down','do you mean that','the lamps had been lit but the blinds had not been drawn','so far i had got when we went to visit the scene of action']\n",
    "for s in sl:\n",
    "    print('\\nSentence: {}'.format(s))\n",
    "    print(\"Probability:{}\".format(probability_sentence_add1_bigram(s)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Good Turing Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1-Gram Model:\n",
      "\n",
      "\tCount= 1 -> Good Turing Count= 0.6346782988004362\n",
      "\tCount= 2 -> Good Turing Count= 1.6546391752577319\n",
      "\tCount= 3 -> Good Turing Count= 2.2866043613707165\n",
      "\tCount= 4 -> Good Turing Count= 3.7329700272479562\n",
      "\tCount= 5 -> Good Turing Count= 4.532846715328467\n",
      "\tCount= 6 -> Good Turing Count= 4.8019323671497585\n",
      "\tCount= 7 -> Good Turing Count= 7.154929577464789\n",
      "\tCount= 8 -> Good Turing Count= 7.086614173228346\n",
      "\tCount= 9 -> Good Turing Count= 7.5\n",
      "\tCount= 10 -> Good Turing Count= 9.68\n",
      "\n",
      " Discounting value d for 1-gram= 0.5934785304151798\n",
      "\n",
      "\n",
      "2-Gram Model:\n",
      "\n",
      "\tCount= 1 -> Good Turing Count= 0.32000756072204894\n",
      "\tCount= 2 -> Good Turing Count= 1.1139988186650915\n",
      "\tCount= 3 -> Good Turing Count= 1.9533404029692472\n",
      "\tCount= 4 -> Good Turing Count= 2.8718783930510314\n",
      "\tCount= 5 -> Good Turing Count= 4.378071833648393\n",
      "\tCount= 6 -> Good Turing Count= 4.461139896373057\n",
      "\tCount= 7 -> Good Turing Count= 6.861788617886178\n",
      "\tCount= 8 -> Good Turing Count= 6.5260663507109005\n",
      "\tCount= 9 -> Good Turing Count= 8.03921568627451\n",
      "\tCount= 10 -> Good Turing Count= 7.780487804878049\n",
      "\n",
      " Discounting value d for 2-gram= 1.0694004634821492\n",
      "\n",
      "\n",
      "3-Gram Model:\n",
      "\n",
      "\tCount= 1 -> Good Turing Count= 0.1348203459059289\n",
      "\tCount= 2 -> Good Turing Count= 0.8115976331360947\n",
      "\tCount= 3 -> Good Turing Count= 1.6727909011373578\n",
      "\tCount= 4 -> Good Turing Count= 2.688284518828452\n",
      "\tCount= 5 -> Good Turing Count= 3.0583657587548636\n",
      "\tCount= 6 -> Good Turing Count= 6.305343511450381\n",
      "\tCount= 7 -> Good Turing Count= 4.5423728813559325\n",
      "\tCount= 8 -> Good Turing Count= 6.7164179104477615\n",
      "\tCount= 9 -> Good Turing Count= 6.0\n",
      "\tCount= 10 -> Good Turing Count= 9.166666666666666\n",
      "\n",
      " Discounting value d for 3-gram= 1.390333987231656\n",
      "\n",
      "\n",
      "4-Gram Model:\n",
      "\n",
      "\tCount= 1 -> Good Turing Count= 0.046251818506217235\n",
      "\tCount= 2 -> Good Turing Count= 0.5839762611275965\n",
      "\tCount= 3 -> Good Turing Count= 1.2804878048780488\n",
      "\tCount= 4 -> Good Turing Count= 2.9047619047619047\n",
      "\tCount= 5 -> Good Turing Count= 1.6721311475409837\n",
      "\tCount= 6 -> Good Turing Count= 7.823529411764706\n",
      "\tCount= 7 -> Good Turing Count= 6.7368421052631575\n",
      "\tCount= 8 -> Good Turing Count= 3.9375\n",
      "\tCount= 9 -> Good Turing Count= 4.285714285714286\n",
      "\tCount= 10 -> Good Turing Count= 7.333333333333333\n",
      "\n",
      " Discounting value d for 4-gram= 1.8395471927109768\n",
      "\n"
     ]
    }
   ],
   "source": [
    "freq_count_ngram = {1:{},2:{},3:{},4:{}}\n",
    "dict_ngram_sorted_by_count = {1:{},2:{},3:{},4:{}}\n",
    "for i in range(1,5):\n",
    "    dict_ngram_sorted_by_count[i] = sorted(dict_wordseq_count[i].items(), key=lambda value: value[1], reverse=True)\n",
    "\n",
    "for i in range(1,5):\n",
    "    for k in dict_ngram_sorted_by_count[i]:\n",
    "        try:\n",
    "            freq_count_ngram[i][k[1]]+=1\n",
    "        except KeyError:\n",
    "            freq_count_ngram[i].update({k[1]:1})\n",
    "\n",
    "            \n",
    "# print(dict_ngram_sorted_by_count[2])\n",
    "# print(freq_count_ngram[1])\n",
    "\n",
    "good_turing_counts = {1:{},2:{},3:{},4:{}}\n",
    "# good_turing_prob = {1:{},2:{},3:{},4:{}}\n",
    "for i in range(1,5):\n",
    "    for k in freq_count_ngram[i]:\n",
    "        try:\n",
    "            c_new_gt = (k+1)*freq_count_ngram[i][k+1]/(freq_count_ngram[i][k])\n",
    "            good_turing_counts[i].update({k: c_new_gt})\n",
    "#             good_turing_prob[i].update({k: c_new_gt/(len(dict_ngram[i])) })\n",
    "        except KeyError:\n",
    "            good_turing_counts[i].update({k: 0})\n",
    "#             good_turing_prob[i].update({k: 0})\n",
    "    \n",
    "# print(good_turing_counts)\n",
    "d_good_turing = {}\n",
    "for k in range(1,5):\n",
    "    s = 0\n",
    "    print(\"\\n{}-Gram Model:\\n\".format(k))\n",
    "    for i in range(1,11):\n",
    "        print(\"\\tCount= {} -> Good Turing Count= {}\".format(i,good_turing_counts[k][i]))\n",
    "        s += i-good_turing_counts[k][i]\n",
    "    s = s/10\n",
    "    print(\"\\n Discounting value d for {}-gram= {}\\n\".format(k,s))\n",
    "    d_good_turing.update({k:s})  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Good Turing Probabilities for Bigrams using Discounting value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = d_good_turing[2]\n",
    "prob_gt_bigram = {}\n",
    "for k in range(max(freq_count_ngram[2].keys())):\n",
    "    try:\n",
    "        if freq_count_ngram[2][k]>d:\n",
    "            prob_gt_bigram.update({k: (freq_count_ngram[2][k]-d)/(len(dict_ngram[2]))})\n",
    "        else:\n",
    "            prob_gt_bigram.update({k: (freq_count_ngram[2][1])/(len(dict_ngram[2]))})\n",
    "    except KeyError:\n",
    "        prob_gt_bigram.update({k: (freq_count_ngram[2][1])/(len(dict_ngram[2]))})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre processing to find Perplexity for Bigram with Good Turing Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=2\n",
    "list_bigram_testing = []\n",
    "for w in words_sent_corpus_testing:\n",
    "    for w2 in list(ngrams(w,n)):\n",
    "        if n==1:\n",
    "            list_ngram_testing.append((w2))\n",
    "        else:\n",
    "            if w2!=('','','','') and w2!=('','','') and w2!=('','') and w2!=(''):\n",
    "                list_bigram_testing.append(w2)\n",
    "\n",
    "                \n",
    "dict_bigram_count_testing={} #counts of bigrams in testing portion of corpus     \n",
    "if len(dict_bigram_count_testing)!=0: # to ensure this function isn't executed twice in the notebook\n",
    "    pass\n",
    "else:\n",
    "    for i in list_bigram_testing:\n",
    "        try:\n",
    "            dict_bigram_count_testing[i]+=1\n",
    "        except KeyError:\n",
    "            dict_bigram_count_testing.update({i:1})\n",
    "\n",
    "# dict_bigram_count_testing\n",
    "# list_bigram_testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perplexity for Bigram after Add-1 Smoothing and Good Turing Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for Bigram with Add-1 smoothing is:1564.48\n",
      "Perplexity for Bigram with Good Turing smoothing is:27.07\n"
     ]
    }
   ],
   "source": [
    "# finding bigrams in testing\n",
    "list_unique_bigram_testing=[]\n",
    "list_words=[]\n",
    "for sent in words_sent_corpus_testing:\n",
    "    l_temp=[]\n",
    "    for i in range(len(sent)-1):\n",
    "        if (sent[i],sent[i+1]) not in list_unique_bigram_testing:\n",
    "            list_unique_bigram_testing.append((sent[i],sent[i+1]))\n",
    "        l_temp.append(sent[i])\n",
    "    list_words.append(l_temp)\n",
    "\n",
    "c = 0 # total no of bigrams\n",
    "c2 = 0\n",
    "perp = 0\n",
    "perp_gt = 0\n",
    "for s in list_words:\n",
    "    for i in range(len(s)-1):\n",
    "        c += 1\n",
    "        p = add1_bigram_prob(tuple([s[i],s[i+1]]))\n",
    "        try:\n",
    "            pgt = prob_gt_bigram[dict_bigram_count_testing[tuple([s[i],s[i+1]])]]\n",
    "        except KeyError:\n",
    "            continue\n",
    "        perp += math.log(p)\n",
    "        perp_gt += math.log(pgt)\n",
    "\n",
    "# print(perp_t)\n",
    "# print('len(list_bigrams_testing):',len(list_bigrams_testing))\n",
    "# print('len(list_words):',len(list_words))\n",
    "perp = perp*(-1)/c\n",
    "perp = math.exp(perp)\n",
    "\n",
    "perp_gt = perp_gt*(-1)/c\n",
    "perp_gt = math.exp(perp_gt)\n",
    "\n",
    "print(\"Perplexity for Bigram with Add-1 smoothing is:{}\".format(round(perp,2)))\n",
    "print(\"Perplexity for Bigram with Good Turing smoothing is:{}\".format(round(perp_gt,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
